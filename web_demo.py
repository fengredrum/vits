import streamlit as st
import torch
import numpy as np

import utils
import commons
from models import SynthesizerTrn
from text.symbols import symbols
from text import text_to_sequence

st.set_page_config(
    page_title="Hello",
    page_icon="ğŸ‘‹",
)

def load_model_configs():
    config_path = "./configs/mix_base.json"
    model_path = "./saved/G_118000.pth"

    hps_ms = utils.get_hparams_from_file(config_path)
    net_g_ms = SynthesizerTrn(
        len(symbols),
        hps_ms.data.filter_length // 2 + 1,
        hps_ms.train.segment_size // hps_ms.data.hop_length,
        n_speakers=hps_ms.data.n_speakers,
        **hps_ms.model)
    net_g_ms.eval()
    utils.load_checkpoint(model_path, net_g_ms, None)
    return net_g_ms, hps_ms

def get_text(text, hps):
    text_norm = text_to_sequence(text, hps.data.text_cleaners)
    if hps.data.add_blank:
        text_norm = commons.intersperse(text_norm, 0)
    text_norm = torch.LongTensor(text_norm)
    return text_norm

def inference(net_g_ms, hps_ms, text_norm, sid=4):
    sid = torch.LongTensor([sid])  # speaker identity
    stn_tst = get_text(text_norm, hps_ms)

    with torch.no_grad():
        x_tst = stn_tst.unsqueeze(0)
        x_tst_lengths = torch.LongTensor([stn_tst.size(0)])
        audio = net_g_ms.infer(x_tst, x_tst_lengths, sid=sid,
                            noise_scale=.667, noise_scale_w=0.8, length_scale=1)[0][0]
    return audio.data.cpu().float().numpy()

samples = [
        "å®œå®¶å””ç³»äº‹å¿…è¦ä½ è®²ï¼Œä½†ç³»ä½ æ‰€è®²å˜…è¯´è¯å°†ä¼šå˜æˆå‘ˆå ‚è¯ä¾›ã€‚",
        "å„ä¸ªå›½å®¶æœ‰å„ä¸ªå›½å®¶å˜…å›½æ­Œ",
        "å„ä¸ªå›½å®¶æœ‰å„ä¸ªå›½å®¶çš„å›½æ­Œ",
        "ä¸ºç ”åˆ¤æœªæ¥ç§‘æŠ€å‘å±•è¶‹åŠ¿ã€å‰ç»è°‹åˆ’å’Œå¸ƒå±€å‰æ²¿ç§‘æŠ€é¢†åŸŸä¸æ–¹å‘æä¾›ä¾æ®",
    ]

if __name__ == "__main__":

    st.write("# è¯­éŸ³åˆæˆæ¼”ç¤º")
    selected_dialog = st.selectbox(
        "è¯·é€‰æ‹©æ¨¡æ¿æˆ–ç›´æ¥è¾“å…¥æ–‡å­—",
        ('æ— ', 'ç²¤è¯­æ¨¡æ¿1', 'ç²¤è¯­æ¨¡æ¿2', 'æ™®é€šè¯æ¨¡æ¿1', 'æ™®é€šè¯æ¨¡æ¿2', 'æ··è¯»'))

    mandarin_text = None
    cantonese_text = None
    if selected_dialog == 'ç²¤è¯­æ¨¡æ¿1':
        cantonese_text = samples[0]
    elif selected_dialog == 'ç²¤è¯­æ¨¡æ¿2':
        cantonese_text = samples[1]
    elif selected_dialog == 'æ™®é€šè¯æ¨¡æ¿1':
        mandarin_text = samples[2]
    elif selected_dialog == 'æ™®é€šè¯æ¨¡æ¿2':
        mandarin_text = samples[3]
    elif selected_dialog == 'æ··è¯»':
        cantonese_text = samples[1]
        mandarin_text = samples[2]

    mandarin_text = st.text_area(
        "æ™®é€šè¯",
        mandarin_text,
        height=50,
    )
    cantonese_text = st.text_area(
        "ç²¤è¯­",
        cantonese_text,
        height=50,
    )
    
    if mandarin_text != 'None':
        mandarin_text = '[ZH]' + mandarin_text + '[ZH]'
    else:
        mandarin_text = ''

    if cantonese_text != 'None':
        cantonese_text = '[GD]' + cantonese_text + '[GD]'
    else:
        cantonese_text = ''
    text_norm = mandarin_text + cantonese_text

    speakers = []
    for i in range(100):
        if i < 10:
            speaker = '0' + str(i)
        else:
            speaker = str(i)
        speakers.append(speaker)
    
    sid = st.radio(
        "è¯·é€‰æ‹©è¯´è¯äºº",
        speakers,
        horizontal=True
    )

    button_submit = st.button("ç”Ÿæˆ")
    if button_submit:
        if len(text_norm) > 0:
            with st.spinner('æ¨ç†ä¸­...'):
                net_g_ms, hps_ms = load_model_configs()
                audio = inference(net_g_ms, hps_ms, text_norm, sid=int(sid))
            st.audio(audio, sample_rate=hps_ms.data.sampling_rate)

    
    